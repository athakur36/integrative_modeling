explain.models[[2]] <- as.formula(paste(colnames(model.bic.backward$model)[1], "~", paste(colnames(model.bic.backward$model)[-1], collapse = " + ")))
explain.models[[2]]
nvmax = 26, method = "exhaustive")
sub_set <- regsubsets(unlist(explain.X[ncol(explain.X)])~., data= explain.X, nbest = 1,
nvmax = 26, method = "exhaustive")
sum_sub <- summary(sub_set)
#sum_sub
plot(sub_set)
# saving best models based on different criterias (R2, Adj. R2, CP, BIC)
result<-data.frame(
Adj.R2 = which.max(sum_sub$adjr2)
)
# model selection using adjested rsqr criterion
all.pred.status <- summary(sub_set)$which[result$Adj.R2,-1]
predictors <- names(which(all.pred.status == TRUE))
predictors <- paste(predictors, collapse = "+")
explain.models[[3]] <-as.formula(paste0(, "~", predictors))
predictors
all.pred.status <- summary(sub_set)$which[result$Adj.R2,-1]
predictors <- names(which(all.pred.status == TRUE))
predictors <- paste(predictors, collapse = "+")
predictors
all.pred.status
sub_set <- regsubsets(DV4~., data= explain.X, nbest = 1,
nvmax = 26, method = "exhaustive")
sum_sub <- summary(sub_set)
#sum_sub
plot(sub_set)
# saving best models based on different criterias (R2, Adj. R2, CP, BIC)
result<-data.frame(
Adj.R2 = which.max(sum_sub$adjr2)
)
# model selection using adjested rsqr criterion
all.pred.status <- summary(sub_set)$which[result$Adj.R2,-1]
predictors <- names(which(all.pred.status == TRUE))
predictors <- paste(predictors, collapse = "+")
explain.models[[3]] <-as.formula(paste0(, "~", predictors))
all.pred.status
all.pred.status[-1]
predictors
all.pred.status <- summary(sub_set)$which[result$Adj.R2,-1]
predictors <- names(which(all.pred.status == TRUE))
predictors <- paste(predictors, collapse = "+")
explain.models[[3]] <-as.formula(paste0(colnames(model.bic.backward$model)[1], "~", predictors))
explain.models[[3]]
set.seed(123)
# Set up repeated k-fold cross-validation
train.control <- trainControl(method = "repeatedcv", number = 3, repeats = 5)
# Train the model
step.model <- train(DV4 ~., data = explain.X,
method = "leapBackward",
tuneGrid = data.frame(nvmax = 1:26),
trControl = train.control
)
step.model$results
#applying 1 standard error rule
min.rmse <- min(step.model$results$RMSE)
min.rmsesd <- step.model$results[which.min(step.model$results$RMSE),'RMSESD']
rmse.cutoff <- min.rmse + min.rmsesd
considered.models <- step.model$results[step.model$results$RMSE<rmse.cutoff,]
predictors <- paste(names(coef(step.model$finalModel,considered.models$nvmax[1]))[-1], collapse = "+")
predictors
names(coef(step.model$finalModel,considered.models$nvmax[1]))
step.model$results
colnames(model.bic.backward$model)[1]
colnames(model.bic.backward$model)
set.seed(123)
# Set up repeated k-fold cross-validation
train.control <- trainControl(method = "repeatedcv", number = 3, repeats = 5)
# Train the model
step.model <- train(DV4 ~., data = explain.X,
method = "leapBackward",
tuneGrid = data.frame(nvmax = 1:26),
trControl = train.control
)
step.model$results
#applying 1 standard error rule
min.rmse <- min(step.model$results$RMSE)
min.rmsesd <- step.model$results[which.min(step.model$results$RMSE),'RMSESD']
rmse.cutoff <- min.rmse + min.rmsesd
considered.models <- step.model$results[step.model$results$RMSE<rmse.cutoff,]
predictors <- paste(names(coef(step.model$finalModel,considered.models$nvmax[1]))[-1], collapse = "+")
explain.models[[4]] <-as.formula(paste0(colnames(model.bic.backward$model)[1], "~", predictors))
predict.X <- set.predict %>%
dplyr::select(IV1, IV2, IV3, IV4, IV5, IV6, IV7, IV8, IV9, IV10, IV11, IV12, IV13, IV14, IV15, IV16, IV17, IV18, IV19, IV20, IV21, IV22, IV23, IV24, IV29, IV28, DV4)
# Set seed for reproducibility
set.seed(123)
# Set up repeated k-fold cross-validation
# repeated k-fold repeats the k-fold cross-validation for the desired times for reilable accuracy
# I tried different vales of K and repeats and fimally settled for k=5, repeats=3
train.control <- trainControl(method = "repeatedcv", number = 3, repeats = 5)
# Train the model with all the predictors
step.model0 <- train(DV4 ~ ., data = predict.X, method = "lm", trControl = train.control)
?for
for (each in explain.models){
typeof(explain.models)
# testing with the three models (model2 and model4 are same) from "explain" step
for (each in explain.models){
predict.models[[each]]<- train(explain.models[[each]], data = predict.X, method = "lm", trControl = train.control)
}
each
# testing with the three models (model2 and model4 are same) from "explain" step
for (each in 1:4){
predict.models[[each]]<- train(explain.models[[each]], data = predict.X, method = "lm", trControl = train.control)
}
each
predict.models
pred.mod.results <- rbind(  predict.models[[1]]$results, predict.models[[2]]$results, predict.models[[3]]$results, predict.models[[4]]$results)
pred.mod.results
final.model <- mod.list[which.min(pred.mod.results$RMSE)]
final.model
predict.models[which.min(pred.mod.results$RMSE)]
predict.models[which.min(pred.mod.results$RMSE)]$finalModel
which.min(pred.mod.results$RMSE)
predict.models[1]
predict.models[2]
pred.mod.results
final.model <- predict.models[which.min(pred.mod.results$RMSE)]
final.model
coef(final.model)
which.min(pred.mod.results$RMSE)
predict.models[[1]]
predict.models[[1]]$pred
predict.models[[1]]$finalModel
predict.models[[which.min(pred.mod.results$RMSE)]]$finalModel
explain.models <- vector(mode="list", length=4)
predict.models <- vector(mode="list", length=4)
explain.X <- set.explain %>%
dplyr::select(IV1, IV2, IV3, IV4, IV5, IV6, IV7, IV8, IV9, IV10, IV11, IV12, IV13, IV14, IV15, IV16, IV17, IV18, IV19, IV20, IV21, IV22, IV23, IV24, IV29, IV28, DV4)
## model1- backward and forward selection using p-value
fitall <- lm(DV4~., data= explain.X)
summary(fitall)
#backward elimination at p>0.05
model.backward.p <- ols_step_backward_p(fitall, prem= 0.05, details = TRUE)
#model.backward.p$model
explain.models[[1]] <- as.formula(paste(colnames(model.backward.p$model$model)[1], "~", paste(colnames(model.backward.p$model$model)[-1], collapse = " + ")))
## model2- using BIC criterion
#backward selection
#here log(n) is used for BIC as elemination criterion
n<- nrow(explain.X)
model.bic.backward<-stepAIC(fitall, scope = list(upper = fitall, lower = ~1), direction = "backward",
k = log(n), trace = TRUE)
explain.models[[2]] <- as.formula(paste(colnames(model.bic.backward$model)[1], "~", paste(colnames(model.bic.backward$model)[-1], collapse = " + ")))
#summary(model.bic.backward)
################## Exhaustive search- adj-rsqr
sub_set <- regsubsets(DV4~., data= explain.X, nbest = 1,
nvmax = 26, method = "exhaustive")
sum_sub <- summary(sub_set)
#sum_sub
plot(sub_set)
# saving best models based on different criterias (R2, Adj. R2, CP, BIC)
result<-data.frame(
Adj.R2 = which.max(sum_sub$adjr2)
)
# model selection using adjested rsqr criterion
all.pred.status <- summary(sub_set)$which[result$Adj.R2,-1]
predictors <- names(which(all.pred.status == TRUE))
predictors <- paste(predictors, collapse = "+")
explain.models[[3]] <-as.formula(paste0(colnames(model.bic.backward$model)[1], "~", predictors))
####################### 1 stnd error rule######
set.seed(123)
# Set up repeated k-fold cross-validation
train.control <- trainControl(method = "repeatedcv", number = 3, repeats = 5)
# Train the model
step.model <- train(DV4 ~., data = explain.X,
method = "leapBackward",
tuneGrid = data.frame(nvmax = 1:26),
trControl = train.control
)
step.model$results
#applying 1 standard error rule
min.rmse <- min(step.model$results$RMSE)
min.rmsesd <- step.model$results[which.min(step.model$results$RMSE),'RMSESD']
rmse.cutoff <- min.rmse + min.rmsesd
considered.models <- step.model$results[step.model$results$RMSE<rmse.cutoff,]
predictors <- paste(names(coef(step.model$finalModel,considered.models$nvmax[1]))[-1], collapse = "+")
explain.models[[4]] <-as.formula(paste0(colnames(model.bic.backward$model)[1], "~", predictors))
plot(c(1:26), step.model$results$RMSE)
abline(h=rmse.cutoff,lty=2)
p<- ggplot(step.model$results, aes(x=nvmax, y=RMSE)) +
geom_line() +
geom_point()+
geom_errorbar(aes(ymin=RMSE-RMSESD, ymax=RMSE+RMSESD), width=.2,
position=position_dodge(0.05))
print(p)
# Finished line plot
p+labs(title="1 se rule", x="number of variables", y = "RMSE")+
theme_classic() +
scale_color_manual(values=c('#999999','#E69F00'))
################################ PREDICTION STEP ###########################
predict.X <- set.predict %>%
dplyr::select(IV1, IV2, IV3, IV4, IV5, IV6, IV7, IV8, IV9, IV10, IV11, IV12, IV13, IV14, IV15, IV16, IV17, IV18, IV19, IV20, IV21, IV22, IV23, IV24, IV29, IV28, DV4)
# Set seed for reproducibility
set.seed(123)
# Set up repeated k-fold cross-validation
# repeated k-fold repeats the k-fold cross-validation for the desired times for reilable accuracy
# I tried different vales of K and repeats and fimally settled for k=3, repeats=5
train.control <- trainControl(method = "repeatedcv", number = 3, repeats = 5)
# testing with the four models from "explain" step
for (each in 1:4){
predict.models[[each]]<- train(explain.models[[each]], data = predict.X, method = "lm", trControl = train.control)
}
# model selection using minimum RMSE criterion
#pred.mod.results <- rbind( step.model4$results, step.model2$results, step.model1$results, step.model3$results, step.model0$results)
pred.mod.results <- rbind(  predict.models[[1]]$results, predict.models[[2]]$results, predict.models[[3]]$results, predict.models[[4]]$results)
#printing final model
predict.models[[which.min(pred.mod.results$RMSE)]]$finalModel
explain.models
pred.mod.results
explain.models <- vector(mode="list", length=4)
predict.models <- vector(mode="list", length=4)
explain.X <- set.explain %>%
dplyr::select(IV1, IV2, IV3, IV4, IV5, IV6, IV7, IV8, IV9, IV10, IV11, IV12, IV13, IV14, IV15, IV16, IV17, IV18, IV19, IV20, IV21, IV22, IV23, IV24, IV29, IV28, DV1)
## model1- backward and forward selection using p-value
fitall <- lm(DV1~., data= explain.X)
summary(fitall)
#backward elimination at p>0.05
model.backward.p <- ols_step_backward_p(fitall, prem= 0.05, details = TRUE)
#model.backward.p$model
explain.models[[1]] <- as.formula(paste(colnames(model.backward.p$model$model)[1], "~", paste(colnames(model.backward.p$model$model)[-1], collapse = " + ")))
## model2- using BIC criterion
#backward selection
#here log(n) is used for BIC as elemination criterion
n<- nrow(explain.X)
model.bic.backward<-stepAIC(fitall, scope = list(upper = fitall, lower = ~1), direction = "backward",
k = log(n), trace = TRUE)
explain.models[[2]] <- as.formula(paste(colnames(model.bic.backward$model)[1], "~", paste(colnames(model.bic.backward$model)[-1], collapse = " + ")))
#summary(model.bic.backward)
################## Exhaustive search- adj-rsqr
sub_set <- regsubsets(DV1~., data= explain.X, nbest = 1,
nvmax = 26, method = "exhaustive")
sum_sub <- summary(sub_set)
#sum_sub
plot(sub_set)
# saving best models based on different criterias (R2, Adj. R2, CP, BIC)
result<-data.frame(
Adj.R2 = which.max(sum_sub$adjr2)
)
# model selection using adjested rsqr criterion
all.pred.status <- summary(sub_set)$which[result$Adj.R2,-1]
predictors <- names(which(all.pred.status == TRUE))
predictors <- paste(predictors, collapse = "+")
explain.models[[3]] <-as.formula(paste0(colnames(model.bic.backward$model)[1], "~", predictors))
####################### 1 stnd error rule######
set.seed(123)
# Set up repeated k-fold cross-validation
train.control <- trainControl(method = "repeatedcv", number = 3, repeats = 5)
# Train the model
step.model <- train(DV1 ~., data = explain.X,
method = "leapBackward",
tuneGrid = data.frame(nvmax = 1:26),
trControl = train.control
)
step.model$results
#applying 1 standard error rule
min.rmse <- min(step.model$results$RMSE)
min.rmsesd <- step.model$results[which.min(step.model$results$RMSE),'RMSESD']
rmse.cutoff <- min.rmse + min.rmsesd
considered.models <- step.model$results[step.model$results$RMSE<rmse.cutoff,]
predictors <- paste(names(coef(step.model$finalModel,considered.models$nvmax[1]))[-1], collapse = "+")
explain.models[[4]] <-as.formula(paste0(colnames(model.bic.backward$model)[1], "~", predictors))
plot(c(1:26), step.model$results$RMSE)
abline(h=rmse.cutoff,lty=2)
p<- ggplot(step.model$results, aes(x=nvmax, y=RMSE)) +
geom_line() +
geom_point()+
geom_errorbar(aes(ymin=RMSE-RMSESD, ymax=RMSE+RMSESD), width=.2,
position=position_dodge(0.05))
print(p)
# Finished line plot
p+labs(title="1 se rule", x="number of variables", y = "RMSE")+
theme_classic() +
scale_color_manual(values=c('#999999','#E69F00'))
################################ PREDICTION STEP ###########################
predict.X <- set.predict %>%
dplyr::select(IV1, IV2, IV3, IV4, IV5, IV6, IV7, IV8, IV9, IV10, IV11, IV12, IV13, IV14, IV15, IV16, IV17, IV18, IV19, IV20, IV21, IV22, IV23, IV24, IV29, IV28, DV1)
# Set seed for reproducibility
set.seed(123)
# Set up repeated k-fold cross-validation
# repeated k-fold repeats the k-fold cross-validation for the desired times for reilable accuracy
# I tried different vales of K and repeats and fimally settled for k=3, repeats=5
train.control <- trainControl(method = "repeatedcv", number = 3, repeats = 5)
# testing with the four models from "explain" step
for (each in 1:4){
predict.models[[each]]<- train(explain.models[[each]], data = predict.X, method = "lm", trControl = train.control)
}
# model selection using minimum RMSE criterion
#pred.mod.results <- rbind( step.model4$results, step.model2$results, step.model1$results, step.model3$results, step.model0$results)
pred.mod.results <- rbind(  predict.models[[1]]$results, predict.models[[2]]$results, predict.models[[3]]$results, predict.models[[4]]$results)
#printing final model
predict.models[[which.min(pred.mod.results$RMSE)]]$finalModel
explain.models
pred.mod.results
explain.models <- vector(mode="list", length=4)
predict.models <- vector(mode="list", length=4)
explain.X <- set.explain %>%
dplyr::select(IV1, IV2, IV3, IV4, IV5, IV6, IV7, IV8, IV9, IV10, IV11, IV12, IV13, IV14, IV15, IV16, IV17, IV18, IV19, IV20, IV21, IV22, IV23, IV24, IV29, IV28, DV2)
## model1- backward and forward selection using p-value
fitall <- lm(DV2~., data= explain.X)
summary(fitall)
#backward elimination at p>0.05
model.backward.p <- ols_step_backward_p(fitall, prem= 0.05, details = TRUE)
#model.backward.p$model
explain.models[[1]] <- as.formula(paste(colnames(model.backward.p$model$model)[1], "~", paste(colnames(model.backward.p$model$model)[-1], collapse = " + ")))
## model2- using BIC criterion
#backward selection
#here log(n) is used for BIC as elemination criterion
n<- nrow(explain.X)
model.bic.backward<-stepAIC(fitall, scope = list(upper = fitall, lower = ~1), direction = "backward",
k = log(n), trace = TRUE)
explain.models[[2]] <- as.formula(paste(colnames(model.bic.backward$model)[1], "~", paste(colnames(model.bic.backward$model)[-1], collapse = " + ")))
#summary(model.bic.backward)
################## Exhaustive search- adj-rsqr
sub_set <- regsubsets(DV2~., data= explain.X, nbest = 1,
nvmax = 26, method = "exhaustive")
sum_sub <- summary(sub_set)
#sum_sub
plot(sub_set)
# saving best models based on different criterias (R2, Adj. R2, CP, BIC)
result<-data.frame(
Adj.R2 = which.max(sum_sub$adjr2)
)
# model selection using adjested rsqr criterion
all.pred.status <- summary(sub_set)$which[result$Adj.R2,-1]
predictors <- names(which(all.pred.status == TRUE))
predictors <- paste(predictors, collapse = "+")
explain.models[[3]] <-as.formula(paste0(colnames(model.bic.backward$model)[1], "~", predictors))
####################### 1 stnd error rule######
set.seed(123)
# Set up repeated k-fold cross-validation
train.control <- trainControl(method = "repeatedcv", number = 3, repeats = 5)
# Train the model
step.model <- train(DV2 ~., data = explain.X,
method = "leapBackward",
tuneGrid = data.frame(nvmax = 1:26),
trControl = train.control
)
step.model$results
#applying 1 standard error rule
min.rmse <- min(step.model$results$RMSE)
min.rmsesd <- step.model$results[which.min(step.model$results$RMSE),'RMSESD']
rmse.cutoff <- min.rmse + min.rmsesd
considered.models <- step.model$results[step.model$results$RMSE<rmse.cutoff,]
predictors <- paste(names(coef(step.model$finalModel,considered.models$nvmax[1]))[-1], collapse = "+")
explain.models[[4]] <-as.formula(paste0(colnames(model.bic.backward$model)[1], "~", predictors))
plot(c(1:26), step.model$results$RMSE)
abline(h=rmse.cutoff,lty=2)
p<- ggplot(step.model$results, aes(x=nvmax, y=RMSE)) +
geom_line() +
geom_point()+
geom_errorbar(aes(ymin=RMSE-RMSESD, ymax=RMSE+RMSESD), width=.2,
position=position_dodge(0.05))
print(p)
# Finished line plot
p+labs(title="1 se rule", x="number of variables", y = "RMSE")+
theme_classic() +
scale_color_manual(values=c('#999999','#E69F00'))
################################ PREDICTION STEP ###########################
predict.X <- set.predict %>%
dplyr::select(IV1, IV2, IV3, IV4, IV5, IV6, IV7, IV8, IV9, IV10, IV11, IV12, IV13, IV14, IV15, IV16, IV17, IV18, IV19, IV20, IV21, IV22, IV23, IV24, IV29, IV28, DV2)
# Set seed for reproducibility
set.seed(123)
# Set up repeated k-fold cross-validation
# repeated k-fold repeats the k-fold cross-validation for the desired times for reilable accuracy
# I tried different vales of K and repeats and fimally settled for k=3, repeats=5
train.control <- trainControl(method = "repeatedcv", number = 3, repeats = 5)
# testing with the four models from "explain" step
for (each in 1:4){
predict.models[[each]]<- train(explain.models[[each]], data = predict.X, method = "lm", trControl = train.control)
}
# model selection using minimum RMSE criterion
#pred.mod.results <- rbind( step.model4$results, step.model2$results, step.model1$results, step.model3$results, step.model0$results)
pred.mod.results <- rbind(  predict.models[[1]]$results, predict.models[[2]]$results, predict.models[[3]]$results, predict.models[[4]]$results)
#printing final model
predict.models[[which.min(pred.mod.results$RMSE)]]$finalModel
explain.models
pred.mod.results
explain.models <- vector(mode="list", length=4)
predict.models <- vector(mode="list", length=4)
explain.X <- set.explain %>%
dplyr::select(IV1, IV2, IV3, IV4, IV5, IV6, IV7, IV8, IV9, IV10, IV11, IV12, IV13, IV14, IV15, IV16, IV17, IV18, IV19, IV20, IV21, IV22, IV23, IV24, IV29, IV28, DV1)
## model1- backward and forward selection using p-value
fitall <- lm(DV1~., data= explain.X)
summary(fitall)
#backward elimination at p>0.05
model.backward.p <- ols_step_backward_p(fitall, prem= 0.05, details = TRUE)
#model.backward.p$model
explain.models[[1]] <- as.formula(paste(colnames(model.backward.p$model$model)[1], "~", paste(colnames(model.backward.p$model$model)[-1], collapse = " + ")))
## model2- using BIC criterion
#backward selection
#here log(n) is used for BIC as elemination criterion
n<- nrow(explain.X)
model.bic.backward<-stepAIC(fitall, scope = list(upper = fitall, lower = ~1), direction = "backward",
k = log(n), trace = TRUE)
explain.models[[2]] <- as.formula(paste(colnames(model.bic.backward$model)[1], "~", paste(colnames(model.bic.backward$model)[-1], collapse = " + ")))
#summary(model.bic.backward)
################## Exhaustive search- adj-rsqr
sub_set <- regsubsets(DV1~., data= explain.X, nbest = 1,
nvmax = 26, method = "exhaustive")
sum_sub <- summary(sub_set)
#sum_sub
plot(sub_set)
unique(data$DV19)
unique(data$DV21)
rm(list=ls(all=TRUE))
library(dplyr)
library(tidyverse)
library(ggplot2)
library(ggrepel)
library(stringr)
library(knitr)
library(CCA)
library(CCP)
library(readxl)
library(caret)
library(heplots)
library(leaps)
library(olsrr)
library(flexmix)
library(Hmisc)
data = read_xlsx("../data/AGGRdata2022_4upload.xlsx")
#summary(data)
set.seed(917829235)
#Spliting the data in "explain" and "predict" sets
index = createDataPartition(data$DV1, p = 0.50, list = FALSE)
set.explain = data[index, ]
set.predict = data[-index, ]
explain.models <- vector(mode="list", length=4)
predict.models <- vector(mode="list", length=4)
explain.X <- set.explain %>%
dplyr::select(IV1, IV2, IV3, IV4, IV5, IV6, IV7, IV8, IV9, IV10, IV11, IV12, IV13, IV14, IV15, IV16, IV17, IV18, IV19, IV20, IV21, IV22, IV23, IV24, IV29, IV28, DV22)
## model1- backward and forward selection using p-value
fitall <- lm(DV22~., data= explain.X)
summary(fitall)
#backward elimination at p>0.05
model.backward.p <- ols_step_backward_p(fitall, prem= 0.05, details = TRUE)
#model.backward.p$model
explain.models[[1]] <- as.formula(paste(colnames(model.backward.p$model$model)[1], "~", paste(colnames(model.backward.p$model$model)[-1], collapse = " + ")))
## model2- using BIC criterion
#backward selection
#here log(n) is used for BIC as elemination criterion
n<- nrow(explain.X)
model.bic.backward<-stepAIC(fitall, scope = list(upper = fitall, lower = ~1), direction = "backward",
k = log(n), trace = TRUE)
explain.models[[2]] <- as.formula(paste(colnames(model.bic.backward$model)[1], "~", paste(colnames(model.bic.backward$model)[-1], collapse = " + ")))
#summary(model.bic.backward)
################## Exhaustive search- adj-rsqr
sub_set <- regsubsets(DV22~., data= explain.X, nbest = 1,
nvmax = 26, method = "exhaustive")
sum_sub <- summary(sub_set)
#sum_sub
plot(sub_set)
# saving best models based on different criterias (R2, Adj. R2, CP, BIC)
result<-data.frame(
Adj.R2 = which.max(sum_sub$adjr2)
)
# model selection using adjested rsqr criterion
all.pred.status <- summary(sub_set)$which[result$Adj.R2,-1]
predictors <- names(which(all.pred.status == TRUE))
predictors <- paste(predictors, collapse = "+")
explain.models[[3]] <-as.formula(paste0(colnames(model.bic.backward$model)[1], "~", predictors))
####################### 1 stnd error rule######
set.seed(123)
# Set up repeated k-fold cross-validation
train.control <- trainControl(method = "repeatedcv", number = 3, repeats = 5)
# Train the model
step.model <- train(DV22 ~., data = explain.X,
method = "leapBackward",
tuneGrid = data.frame(nvmax = 1:26),
trControl = train.control
)
step.model$results
#applying 1 standard error rule
min.rmse <- min(step.model$results$RMSE)
min.rmsesd <- step.model$results[which.min(step.model$results$RMSE),'RMSESD']
rmse.cutoff <- min.rmse + min.rmsesd
considered.models <- step.model$results[step.model$results$RMSE<rmse.cutoff,]
predictors <- paste(names(coef(step.model$finalModel,considered.models$nvmax[1]))[-1], collapse = "+")
explain.models[[4]] <-as.formula(paste0(colnames(model.bic.backward$model)[1], "~", predictors))
plot(c(1:26), step.model$results$RMSE)
abline(h=rmse.cutoff,lty=2)
p<- ggplot(step.model$results, aes(x=nvmax, y=RMSE)) +
geom_line() +
geom_point()+
geom_errorbar(aes(ymin=RMSE-RMSESD, ymax=RMSE+RMSESD), width=.2,
position=position_dodge(0.05))
print(p)
# Finished line plot
p+labs(title="1 se rule", x="number of variables", y = "RMSE")+
theme_classic() +
scale_color_manual(values=c('#999999','#E69F00'))
################################ PREDICTION STEP ###########################
predict.X <- set.predict %>%
dplyr::select(IV1, IV2, IV3, IV4, IV5, IV6, IV7, IV8, IV9, IV10, IV11, IV12, IV13, IV14, IV15, IV16, IV17, IV18, IV19, IV20, IV21, IV22, IV23, IV24, IV29, IV28, DV22)
# Set seed for reproducibility
set.seed(123)
# Set up repeated k-fold cross-validation
# repeated k-fold repeats the k-fold cross-validation for the desired times for reilable accuracy
# I tried different vales of K and repeats and fimally settled for k=3, repeats=5
train.control <- trainControl(method = "repeatedcv", number = 3, repeats = 5)
# testing with the four models from "explain" step
for (each in 1:4){
predict.models[[each]]<- train(explain.models[[each]], data = predict.X, method = "lm", trControl = train.control)
}
# model selection using minimum RMSE criterion
#pred.mod.results <- rbind( step.model4$results, step.model2$results, step.model1$results, step.model3$results, step.model0$results)
pred.mod.results <- rbind(  predict.models[[1]]$results, predict.models[[2]]$results, predict.models[[3]]$results, predict.models[[4]]$results)
#printing final model
predict.models[[which.min(pred.mod.results$RMSE)]]$finalModel
predict.models[[which.min(pred.mod.results$RMSE)]]$finalModel
pred.mod.results
explain.models
